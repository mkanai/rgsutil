% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read_gsfiles.R
\name{read_gsfiles}
\alias{read_gsfiles}
\alias{map_dfr_gsfiles}
\alias{map_dfc_gsfiles}
\title{Read multiple files from Google Cloud Storage}
\usage{
read_gsfiles(
  remote_pattern,
  func = NULL,
  combine = c("none", "rows", "cols"),
  cache.dir = NULL,
  parallel = FALSE,
  .progress = TRUE,
  update_check_threshold = 100,
  ...
)

map_dfr_gsfiles(remote_pattern, func, ...)

map_dfc_gsfiles(remote_pattern, func, ...)
}
\arguments{
\item{remote_pattern}{Character string or vector. Google Cloud Storage paths or patterns
starting with "gs://". Accepts:
\itemize{
  \item Single pattern: \code{"gs://bucket/data/*.csv"}
  \item Vector of paths: \code{c("gs://bucket/file1.csv", "gs://bucket/file2.csv")}
  \item Brace expansion: \code{"gs://bucket/data/{jan,feb,mar}.csv"}
  \item Mixed patterns: \code{c("gs://bucket/2023/*.csv", "gs://bucket/2024/*.csv")}
}}

\item{func}{Function. Optional function to apply to each file after reading.
Must accept two arguments: \code{df} (the data frame) and \code{path} (the GS file path).
Example: \code{function(df, path) \{ df$source <- basename(path); return(df) \}}.
If NULL, returns raw data frames.}

\item{combine}{Character string. How to combine results: "none" (list),
"rows" (rbind), or "cols" (cbind). Defaults to "none".}

\item{cache.dir}{Character string. Directory for caching downloaded files.
Defaults to getOption("rgsutil.cache_dir") or a temp directory.}

\item{parallel}{Logical or integer. Whether to read files in parallel using future.
If TRUE, uses all available cores. If integer, uses that many cores. Defaults to FALSE.}

\item{.progress}{Logical. Whether to show progress messages. Defaults to TRUE.}

\item{update_check_threshold}{Integer. Number of files above which update checks are skipped
for performance (re-downloading all files is typically faster than checking many files).
Defaults to 100. Set to \code{Inf} to always check for updates, or \code{0} to always skip checks.}

\item{...}{Additional arguments passed to \code{\link{fread_wrapper}} for each file.}
}
\value{
Depending on the \code{combine} parameter:
  \itemize{
    \item "none": A named list of data frames (names are file paths)
    \item "rows": A single data frame with all rows combined
    \item "cols": A single data frame with all columns combined
  }
}
\description{
Downloads and reads multiple files from Google Cloud Storage matching patterns.
Optimizes performance by batch downloading files before reading.
}
\details{
This function optimizes reading multiple files by:
\enumerate{
  \item Listing all files matching the patterns using gcloud's native pattern expansion
  \item Checking which files need updating (if cached) - skipped when file count exceeds
        the \code{update_check_threshold} parameter for performance
  \item Batch downloading all required files using gcloud's native parallel support
  \item Reading and optionally processing each file (optionally in parallel)
  \item Combining results based on the specified method
}

The \code{func} parameter allows custom processing of each file. The function
must accept exactly two arguments: \code{df} (the data frame read from the file)
and \code{path} (the full GS path of the file, e.g., "gs://bucket/file.csv").
The function should return a data frame. This is useful for adding metadata,
filtering data, or extracting information from the file path.

When \code{parallel = TRUE}, the package will use the \code{furrr}
package for parallel reading. Install \code{furrr} and \code{future}
packages to enable parallel processing. For sequential processing with
progress bars, install the \code{purrr} package.

For performance optimization with large file sets, the function
automatically skips individual update checks when the number of files
exceeds \code{update_check_threshold} and re-downloads all files instead.
This is typically faster as checking timestamps for many files individually
can be slower than batch downloading.
}
\examples{
\dontrun{
# Read all CSV files and return a list
data_list <- read_gsfiles("gs://my-bucket/data/*.csv")

# Read specific files
specific_files <- read_gsfiles(c(
  "gs://my-bucket/data/file1.csv",
  "gs://my-bucket/data/file2.csv",
  "gs://my-bucket/data/file3.csv"
))

# Use brace expansion for months
monthly_data <- read_gsfiles(
  "gs://my-bucket/reports/2024-{01,02,03,04,05,06}.csv",
  combine = "rows"
)

# Or more concisely for named months
quarterly_data <- read_gsfiles(
  "gs://my-bucket/reports/2024-{jan,feb,mar}.csv",
  combine = "rows"
)

# Nested brace expansion for multiple years and months
historical_data <- read_gsfiles(
  "gs://my-bucket/reports/{2022,2023,2024}-{jan,feb,mar}.csv",
  combine = "rows"
)

# Mix different patterns
all_data <- read_gsfiles(c(
  "gs://my-bucket/2023/*.csv",
  "gs://my-bucket/2024/*.csv",
  "gs://my-bucket/archive/backup-*.csv"
), combine = "rows")

# Add source file information to each data frame
with_source <- read_gsfiles(
  "gs://my-bucket/logs/2024-*.txt",
  func = function(df, path) {
    df$source_file <- basename(path)
    df$date <- as.Date(gsub(".*-(\\\\d{4}-\\\\d{2}-\\\\d{2})\\\\.txt", "\\\\1", path))
    return(df)
  },
  combine = "rows"
)

# Process genomics files with filtering
variants <- read_gsfiles(
  "gs://genomics/chr*.vcf.bgz",
  func = function(df, path) {
    df \%>\%
      filter(QUAL > 30) \%>\%
      mutate(chromosome = gsub(".*chr(\\\\w+)\\\\..*", "\\\\1", basename(path)))
  },
  combine = "rows",
  sep = "\t"
)

# Use parallel processing for faster reading
# Requires: install.packages(c("furrr", "future"))
large_dataset <- read_gsfiles(
  "gs://my-bucket/big-data/*.parquet",
  parallel = TRUE, # Use all available cores
  combine = "rows"
)

# Specify number of parallel workers
results <- read_gsfiles(
  "gs://my-bucket/data/file*.csv",
  parallel = 4, # Use 4 cores
  func = function(df, path) {
    # Complex processing that benefits from parallelization
    df \%>\%
      group_by(category) \%>\%
      summarise(mean_value = mean(value, na.rm = TRUE))
  }
)

# For many files, skip update checks for better performance
large_dataset <- read_gsfiles(
  "gs://my-bucket/thousands-of-files/*.csv",
  update_check_threshold = 50, # Skip checks if >50 files
  combine = "rows"
)

# Always check for updates regardless of file count
always_check <- read_gsfiles(
  "gs://my-bucket/data/*.csv",
  update_check_threshold = Inf, # Always check
  combine = "rows"
)
}

}
\seealso{
\code{\link{read_gsfile}}, \code{\link{list_gsfile}}
}
